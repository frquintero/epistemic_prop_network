{
  "layers": [
    {
      "id": "layer1",
      "name": "input layer",
      "description": "input layer",
      "nodes": [
        {
          "id": "input node",
          "name": "Input Node",
          "type": "output_node_1",
          "template_id": "output_node_1",
          "expected_output": "output_node_1",
          "description": "remove biases and give a philosphical context to the following question:",
          "llm_config": {
            "model": "openai/gpt-oss-20b",
            "temperature": 0.8,
            "reasoning_effort": "medium",
            "max_tokens": 8192
          }
        }
      ]
    },
    {
      "id": "layer2",
      "name": "layer 2",
      "description": "middle layer",
      "nodes": [
        {
          "id": "middle node",
          "name": "Middle Node",
          "type": "two_answers",
          "template_id": "two_answers",
          "expected_output": "two_answers",
          "description": "based on the question, give two responses: one with a positive answer (what it is) and the other a negative answer (what it is not)",
          "llm_config": {
            "model": "openai/gpt-oss-20b",
            "temperature": 0.8,
            "reasoning_effort": "medium",
            "max_tokens": 8192
          }
        }
      ]
    },
    {
      "id": "layer3",
      "name": "output layer",
      "description": "output layer",
      "nodes": [
        {
          "id": "output node",
          "name": "Output Node",
          "type": "output",
          "template_id": "output",
          "expected_output": "output",
          "description": "integrate and syntethize in one coherent response",
          "llm_config": {
            "model": "openai/gpt-oss-20b",
            "temperature": 0.8,
            "reasoning_effort": "medium",
            "max_tokens": 8192
          }
        }
      ]
    }
  ]
}